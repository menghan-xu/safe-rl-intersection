# ... (保留原有的 Physics, Reward, Safety, PPO Config 部分) ...

# ==========================================
# 1. Physics & Environment
# ==========================================
env_id: "Intersection-Residual-MPC-v0" # 修改 ID
dt: 0.1
max_accel: 3.2
v_limit: 1.5
target_pos: [1.25, 0.0]
robot_radius: 0.3328

# ==========================================
# 2. Reward Weights (Task)
# ==========================================
w_progress: 25
w_overspeed: 72.7
w_comfort: 0.697
reward_success: 120.0
reward_collision: -100.0
w_time_penalty: 0.5

# ==========================================
# 3. Safety Cost Config
# ==========================================
cost_scale_mu: 250
cost_crash: 100.0

# ==========================================
# 4. Lagrangian PPO Config
# ==========================================
cost_limit: 1
lambda_lr: 0.01
lambda_init: 1.0

# ==========================================
# 5. Training Hyperparameters
# ==========================================
seed: 42
epochs: 10
num_envs: 4
num_steps: 200
minibatch_size: 64
update_epochs: 10
lr: 0.0003
gamma: 0.99
gae_lambda: 0.95
clip_ratio: 0.2
ent_coef: 0.015
vf_coef: 0.5
max_grad_norm: 0.5
checkpoint: true
save_gif_freq: 50

# ==========================================
# 6. MPC Parameters (NEW)
# ==========================================
# Prediction horizon steps (N * dt = duration)
mpc_horizon: 30
# Weights for MPC cost function
mpc_w_track: 1.0
mpc_w_comfort: 0.01
mpc_w_terminal: 10.0
mpc_w_safety_slack: 1.0e+9
# PPO Residual Scaling (Optional)
# If 1.0, PPO output [-max_accel, max_accel] is added directly.
# You might want to scale it down if you want PPO to only make small corrections.
residual_scale: 1.0